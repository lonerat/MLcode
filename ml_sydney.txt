# -*- coding: utf-8 -*-
"""ML_Sydney.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oMFzfHf0TovOISpL8ZEUWMSIDthzUBbi
"""

# Hello!
https://github.com/zclaytor

"""

Install scikit learn library

"""

!pip install scikit-learn

"""# KNN with example data"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.inspection import permutation_importance
from sklearn import datasets

# Step 1: Load the dataset
wine = datasets.load_wine()
#wine = datasets.load_diabetes()
#wine = datasets.load_iris()
#wine = datasets.load_digits()
#wine = datasets.load_linnerud()

X = wine.data  # Features (chemical properties of wine)
y = wine.target  # Labels (types of wine)

# Convert to a DataFrame for better handling
df = pd.DataFrame(X, columns=wine.feature_names)

# Compute the correlation matrix
correlation_matrix = df.corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, xticklabels=wine.feature_names, yticklabels=wine.feature_names)
plt.title("Correlation Matrix of Wine Dataset Features")
plt.show()

# Step 2: Explore the dataset
print("Feature Names:", wine.feature_names)
print("Target Names:", wine.target_names)
print("Dataset Shape:", X.shape)

# Step 3: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Step 4: Normalize the feature values (important for KNN)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 5: Train the KNN model
k = 13  # Number of neighbors
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)

# Step 6: Make predictions
y_pred = knn.predict(X_test)

# Step 7: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
#print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))
#print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Step 8: Perform permutation importance analysis
perm_importance = permutation_importance(knn, X_test, y_test, scoring='accuracy', random_state=42)

# Extract importance and feature names
importance_values = perm_importance.importances_mean
feature_names = wine.feature_names

# Step 9: Visualize feature importance
plt.figure()
sns.barplot(x=importance_values, y=feature_names, palette="viridis")
plt.xlabel('Permutation Importance')
plt.ylabel('Feature Names')
plt.title('Feature Importance for KNN')
plt.tight_layout()
plt.show()

# Plot Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d", xticklabels=wine.target_names, yticklabels=wine.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Step 10: Visualize the results
plt.figure()
k_values = range(1, 100)
accuracies = []
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    accuracies.append(accuracy_score(y_test, knn.predict(X_test)))

plt.plot(k_values, accuracies, marker='o', linestyle='dashed')
plt.xlabel('K (Number of Neighbors)')
plt.ylabel('Accuracy')
plt.title('KNN Accuracy for Different K Values')
plt.show()

# Step 11: 3D Visualization of three random features
fig = plt.figure(figsize=(10,10))
ax = fig.add_subplot(111, projection='3d')

# Select three random features
feature_indices = np.random.choice(X.shape[1], 3, replace=False)
X_3d = X[:, feature_indices]

# Scatter plot
ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=y, cmap='viridis', s=50)

ax.set_xlabel(wine.feature_names[feature_indices[0]])
ax.set_ylabel(wine.feature_names[feature_indices[1]])
ax.set_zlabel(wine.feature_names[feature_indices[2]])
ax.set_title('3D Visualization of Wine Dataset Features')

plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

random_states = range(1, 100)  # Example random states
k_values = range(1, 30) # consider k values from 1 to 30
accuracies_by_random_state = {}

plt.figure(figsize=(10, 6))

#Nested for loop
for random_state in random_states:
  accuracies = []
  for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
  plt.plot(k_values, accuracies, marker='o', linestyle='dashed', label=f'Random State = {random_state}', color = "blue", alpha = 0.02)

plt.xlabel('K (Number of Neighbors)')
plt.ylabel('Accuracy')
plt.title('KNN Accuracy vs. K Value for Different Random States')
#plt.legend()
plt.grid(True)
plt.show()

"""# KNN with TESS data"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.inspection import permutation_importance
from sklearn import datasets

# Load the dataset

file_id = "1Uxk0sDwNBGxQLEijyffwPtTpKoFawsxU"
file_url = f"https://drive.google.com/uc?id={file_id}"

df = pd.read_csv(file_url)
#df.head()

# Drop non-numeric columns and fill NaNs
df_numeric = df.select_dtypes(include=[np.number]).fillna(0)

# Extract features (columns 2 to 28)
feature_columns = df_numeric.columns[1:27]
X = df_numeric[feature_columns]

# Extract labels
label_column_name = "INSPEC"
y = df.get(label_column_name)  # Get the column or None

# Encode labels if they exist
if y is not None:
    y, classes = pd.factorize(y)  # Converts categories to integers
else:
    classes = None

# Compute correlation matrix
correlation_matrix = X.corr()

# Plot correlation matrix
plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True,
            xticklabels=feature_columns, yticklabels=feature_columns)
plt.title("Correlation Matrix of Dataset Features")
plt.show()

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Normalize feature values
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train KNN model
k = 4
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)

# Make predictions
y_pred = knn.predict(X_test)

# Evaluate the model
print("Classification Report:\n", classification_report(y_test, y_pred))

# Permutation importance analysis
perm_importance = permutation_importance(knn, X_test, y_test, scoring='accuracy', random_state=42)

# Feature importance plot
plt.figure()
sns.barplot(x=perm_importance.importances_mean, y=feature_columns, palette="viridis")
plt.xlabel('Permutation Importance')
plt.ylabel('Feature Names')
plt.title('Feature Importance for KNN')
plt.tight_layout()
plt.show()

# Confusion Matrix Plot
conf_matrix = confusion_matrix(y_test, y_pred)

# Ensure 'classes' exists for labels
if classes is None:
    classes = np.unique(y_test)

plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d", xticklabels=classes, yticklabels=classes)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Accuracy vs. K plot
plt.figure()
k_values = range(1, 40)
accuracies = []
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    accuracies.append(accuracy_score(y_test, knn.predict(X_test)))

plt.plot(k_values, accuracies, marker='o', linestyle='dashed')
plt.xlabel('K (Number of Neighbors)')
plt.ylabel('Accuracy')
plt.title('KNN Accuracy for Different K Values')
plt.show()

# 3D Feature Visualization
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')

# Select three random features
feature_indices = np.random.choice(len(feature_columns), 3, replace=False)
X_3d = X.iloc[:, feature_indices].values  # Fix indexing issue

# Scatter plot
sc = ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=y, cmap='viridis', s=50)

# Label axes properly
ax.set_xlabel(feature_columns[feature_indices[0]])
ax.set_ylabel(feature_columns[feature_indices[1]])
ax.set_zlabel(feature_columns[feature_indices[2]])
ax.set_title('3D Feature Visualization')

plt.show()

random_states = range(1, 100)  # Example random states
k_values = range(1, 100) # consider k values from 1 to 30
accuracies_by_random_state = []

plt.figure(figsize=(10, 6))

#Nested for loop
for random_state in random_states:
  accuracies = []
  for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
  accuracies_by_random_state.append(accuracies)

  plt.plot(k_values, accuracies, marker='o', linestyle='dashed', label=f'Random State = {random_state}', color = "blue", alpha = 0.02)

# convert to array
accuracies_by_random_state = np.array(accuracies_by_random_state)

# compute mean accuracy
mean_accuracies = np.mean(accuracies_by_random_state, axis=0)

# compute trailing moving average
window_size = 1
manual_moving_avg = []

for i in range(len(mean_accuracies)):
  avg = np.mean(mean_accuracies[i-window_size+1:i+1])
  manual_moving_avg.append(avg)

# plot moving average curve
plt.plot(k_values, manual_moving_avg, color='red', linewidth=2, label="Manual Moving Average")

plt.xlabel('K (Number of Neighbors)')
plt.ylabel('Accuracy')
plt.title('KNN Accuracy vs. K Value for Different Random States')
#plt.legend()
plt.grid(True)
plt.show()

# best k value
best_k = np.argmax(manual_moving_avg) + 1
print("Best K Value:", best_k)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score

best_test_size = 0
best_f1_score = 0

test_sizes = []
f1_scores = []

for test_size in np.arange(0.05, 0.6, 0.05):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    knn = KNeighborsClassifier(n_neighbors=best_k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    f1 = f1_score(y_test, y_pred, average='weighted')

    test_sizes.append(test_size)
    f1_scores.append(f1)

    if f1 > best_f1_score:
        best_f1_score = f1
        best_test_size = test_size

# Plotting
plt.plot(test_sizes, f1_scores, marker='o')
plt.xlabel('Test Size')
plt.ylabel('F1 Score')
plt.title('F1 Score vs Test Size')
plt.grid(True)
plt.show()

print("Best test_size:", best_test_size)
print("Best F1-score:", best_f1_score)

"""# SVM with example data


"""

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.inspection import permutation_importance
from sklearn import datasets
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
# Step 1: Load the dataset
wine = datasets.load_wine()
X = wine.data  # Features
y = wine.target  # Labels
# Convert features into a DataFrame
df = pd.DataFrame(X, columns=wine.feature_names)
# Step 2: Correlation Matrix
correlation_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True,
            xticklabels=wine.feature_names, yticklabels=wine.feature_names)
plt.title("Correlation Matrix of Wine Dataset Features")
plt.show()
# Step 3: Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
# Step 4: Normalize the Features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Step 5: Train the SVM Model
svm = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)
svm.fit(X_train, y_train)
# Step 6: Make Predictions and Evaluate
y_pred = svm.predict(X_test)
# Accuracy and Classification Report
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d",
            xticklabels=wine.target_names, yticklabels=wine.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()
# Step 7: Feature Importance Analysis
perm_importance = permutation_importance(svm, X_test, y_test, scoring='accuracy', random_state=42)
importance_values = perm_importance.importances_mean
feature_names = wine.feature_names
plt.figure()
sns.barplot(x=importance_values, y=feature_names, palette="viridis")
plt.xlabel('Permutation Importance')
plt.ylabel('Feature Names')
plt.title('Feature Importance for SVM')
plt.tight_layout()
plt.show()
# Step 8: Hyperparameter Tuning (C and Gamma)
C_values = [0.1, 1, 10, 100]
gamma_values = [0.01, 0.1, 1, 10]
results = []
# Test different combinations of C and gamma
for C in C_values:
    for gamma in gamma_values:
        svm = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)
        svm.fit(X_train, y_train)
        acc = accuracy_score(y_test, svm.predict(X_test))
        results.append((C, gamma, acc))
# Visualize hyperparameter results
results_df = pd.DataFrame(results, columns=['C', 'Gamma', 'Accuracy'])
pivot_table = results_df.pivot(index="C", columns="Gamma", values="Accuracy")
plt.figure(figsize=(8, 6))
sns.heatmap(pivot_table, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Accuracy for Different C and Gamma Values")
plt.xlabel("Gamma")
plt.ylabel("C")
plt.show()
# Step 9: 3D Visualization of Features
feature_indices = np.random.choice(X.shape[1], 3, replace=False)
X_3d = X[:, feature_indices]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=y, cmap='viridis', s=50)
ax.set_xlabel(wine.feature_names[feature_indices[0]])
ax.set_ylabel(wine.feature_names[feature_indices[1]])
ax.set_zlabel(wine.feature_names[feature_indices[2]])
ax.set_title('3D Visualization of Wine Dataset Features')
plt.show()

"""# SVM with TESS data"""

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.inspection import permutation_importance
from sklearn import datasets
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# Load the dataset
file_id = "1mYEpq2oaotHNGkA-TS6_0zjRHwG1XUW6"
file_url = f"https://drive.google.com/uc?id={file_id}"

df = pd.read_csv(file_url)
#df.head()

# Drop non-numeric columns and fill NaNs
df_numeric = df.select_dtypes(include=[np.number]).fillna(0)

# Extract features (columns 2 to 28)
feature_columns = df_numeric.columns[1:27]
X = df_numeric[feature_columns]

# Extract labels
label_column_name = "INSPEC"
y = df.get(label_column_name)  # Get the column or None

# Encode labels if they exist
if y is not None:
    y, classes = pd.factorize(y)  # Converts categories to integers
else:
    classes = None

# Compute correlation matrix
correlation_matrix = X.corr()

# Plot correlation matrix
plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True,
            xticklabels=feature_columns, yticklabels=feature_columns)
plt.title("Correlation Matrix of Dataset Features")
plt.show()

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Normalize feature values
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 5: Train the SVM Model
svm = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)
svm.fit(X_train, y_train)

# Step 6: Make Predictions and Evaluate
y_pred = svm.predict(X_test)

# Accuracy and Classification Report
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Plot
conf_matrix = confusion_matrix(y_test, y_pred)

# Ensure 'classes' exists for labels
if classes is None:
    classes = np.unique(y_test)

plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d", xticklabels=classes, yticklabels=classes)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Step 7: Feature Importance Analysis
perm_importance = permutation_importance(svm, X_test, y_test, scoring='accuracy', random_state=42)
importance_values = perm_importance.importances_mean
feature_names = feature_columns
plt.figure()
sns.barplot(x=importance_values, y=feature_names, palette="viridis")
plt.xlabel('Permutation Importance')
plt.ylabel('Feature Names')
plt.title('Feature Importance for SVM')
plt.tight_layout()
plt.show()

# Step 8: Hyperparameter Tuning (C and Gamma)
C_values = [0.1, 1, 10, 100]
gamma_values = [0.01, 0.1, 1, 10]
results = []

# Test different combinations of C and gamma
for C in C_values:
    for gamma in gamma_values:
        svm = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)
        svm.fit(X_train, y_train)
        acc = accuracy_score(y_test, svm.predict(X_test))
        results.append((C, gamma, acc))

# Visualize hyperparameter results
results_df = pd.DataFrame(results, columns=['C', 'Gamma', 'Accuracy'])
pivot_table = results_df.pivot(index="C", columns="Gamma", values="Accuracy")
plt.figure(figsize=(8, 6))
sns.heatmap(pivot_table, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Accuracy for Different C and Gamma Values")
plt.xlabel("Gamma")
plt.ylabel("C")
plt.show()

# 3D Feature Visualization
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')

# Select three random features
feature_indices = np.random.choice(len(feature_columns), 3, replace=False)
X_3d = X.iloc[:, feature_indices].values  # Fix indexing issue

# Scatter plot
sc = ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=y, cmap='viridis', s=50)

# Label axes properly
ax.set_xlabel(feature_columns[feature_indices[0]])
ax.set_ylabel(feature_columns[feature_indices[1]])
ax.set_zlabel(feature_columns[feature_indices[2]])
ax.set_title('3D Feature Visualization')

"""# RF with example data"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.inspection import permutation_importance
from sklearn import datasets
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# Step 1: Load the dataset
wine = datasets.load_wine()
X = wine.data  # Features
y = wine.target  # Labels

# Convert features into a DataFrame for analysis
df = pd.DataFrame(X, columns=wine.feature_names)

# Step 2: Correlation Matrix
correlation_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True,
            xticklabels=wine.feature_names, yticklabels=wine.feature_names)
plt.title("Correlation Matrix of Wine Dataset Features")
plt.show()

# Step 3: Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Step 4: Normalize the Features (optional for Random Forest but done here for consistency)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 5: Train the Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Step 6: Make Predictions and Evaluate
y_pred = rf.predict(X_test)

# Accuracy and Classification Report
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d",
            xticklabels=wine.target_names, yticklabels=wine.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Step 7: Feature Importance Analysis
feature_importances = rf.feature_importances_
feature_names = wine.feature_names

# Visualize feature importance
plt.figure()
sns.barplot(x=feature_importances, y=feature_names, palette="viridis")
plt.xlabel('Feature Importance')
plt.ylabel('Feature Names')
plt.title('Feature Importance for Random Forest Classifier')
plt.tight_layout()
plt.show()

# Step 8: Hyperparameter Tuning with n_estimators and max_depth
n_estimators_values = [10, 50, 100, 200]
max_depth_values = [None, 5, 10, 20]
results = []

# Test different combinations of n_estimators and max_depth
for n_estimators in n_estimators_values:
    for max_depth in max_depth_values:
        rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
        rf.fit(X_train, y_train)
        acc = accuracy_score(y_test, rf.predict(X_test))
        results.append((n_estimators, max_depth, acc))

# Visualize hyperparameter results
results_df = pd.DataFrame(results, columns=['n_estimators', 'max_depth', 'Accuracy'])
pivot_table = results_df.pivot(index="n_estimators", columns="max_depth", values="Accuracy")

plt.figure(figsize=(8, 6))
sns.heatmap(pivot_table, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Accuracy for Different n_estimators and max_depth")
plt.xlabel("max_depth")
plt.ylabel("n_estimators")
plt.show()

# Step 9: 3D Visualization of Features
feature_indices = np.random.choice(X.shape[1], 3, replace=False)
X_3d = X[:, feature_indices]

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=y, cmap='viridis', s=50)

ax.set_xlabel(wine.feature_names[feature_indices[0]])
ax.set_ylabel(wine.feature_names[feature_indices[1]])
ax.set_zlabel(wine.feature_names[feature_indices[2]])
ax.set_title('3D Visualization of Wine Dataset Features')

plt.show()

"""# RF with TESS data"""

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.inspection import permutation_importance
from sklearn import datasets
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# Load the dataset
file_id = "1mYEpq2oaotHNGkA-TS6_0zjRHwG1XUW6"
file_url = f"https://drive.google.com/uc?id={file_id}"

df = pd.read_csv(file_url)
#df.head()

# Drop non-numeric columns and fill NaNs
df_numeric = df.select_dtypes(include=[np.number]).fillna(0)

# Extract features (columns 2 to 28)
feature_columns = df_numeric.columns[1:27]
X = df_numeric[feature_columns]

# Extract labels
label_column_name = "INSPEC"
y = df.get(label_column_name)  # Get the column or None

# Encode labels if they exist
if y is not None:
    y, classes = pd.factorize(y)  # Converts categories to integers
else:
    classes = None

# Compute correlation matrix
correlation_matrix = X.corr()

# Plot correlation matrix
plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True,
            xticklabels=feature_columns, yticklabels=feature_columns)
plt.title("Correlation Matrix of Dataset Features")
plt.show()

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Normalize feature values
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 5: Train the Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Step 6: Make Predictions and Evaluate
y_pred = rf.predict(X_test)

# Accuracy and Classification Report
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Plot
conf_matrix = confusion_matrix(y_test, y_pred)

# Ensure 'classes' exists for labels
if classes is None:
    classes = np.unique(y_test)

plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d", xticklabels=classes, yticklabels=classes)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Step 7: Feature Importance Analysis
perm_importance = permutation_importance(rf, X_test, y_test, scoring='accuracy', random_state=42)
importance_values = perm_importance.importances_mean
feature_names = feature_columns
plt.figure()
sns.barplot(x=importance_values, y=feature_names, palette="viridis")
plt.xlabel('Permutation Importance')
plt.ylabel('Feature Names')
plt.title('Feature Importance for RF')
plt.tight_layout()
plt.show()

# Step 8: Hyperparameter Tuning with n_estimators and max_depth
n_estimators_values = [10, 50, 100, 200]
max_depth_values = [1, 5, 6, 7, 8, 9, 10, 20, 35, 50]
results = []

# Test different combinations of n_estimators and max_depth
for n_estimators in n_estimators_values:
    for max_depth in max_depth_values:
        rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
        rf.fit(X_train, y_train)
        acc = accuracy_score(y_test, rf.predict(X_test))
        results.append((n_estimators, max_depth, acc))

# Visualize hyperparameter results
results_df = pd.DataFrame(results, columns=['n_estimators', 'max_depth', 'Accuracy'])
pivot_table = results_df.pivot(index="n_estimators", columns="max_depth", values="Accuracy")

plt.figure(figsize=(8, 6))
sns.heatmap(pivot_table, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Accuracy for Different n_estimators and max_depth")
plt.xlabel("max_depth")
plt.ylabel("n_estimators")
plt.show()

# 3D Feature Visualization
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')

# Select three random features
feature_indices = np.random.choice(len(feature_columns), 3, replace=False)
X_3d = X.iloc[:, feature_indices].values  # Fix indexing issue

# Scatter plot
sc = ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=y, cmap='viridis', s=50)

# Label axes properly
ax.set_xlabel(feature_columns[feature_indices[0]])
ax.set_ylabel(feature_columns[feature_indices[1]])
ax.set_zlabel(feature_columns[feature_indices[2]])
ax.set_title('3D Feature Visualization')

"""### Storing ML models to reuse them in other machines or share with people"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from joblib import dump, load
import seaborn as sns
from sklearn import datasets

# Step 1: Load the dataset
wine = datasets.load_wine()
X = wine.data
y = wine.target

# Step 2: Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Step 3: Normalize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 4: Train the Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Step 5: Save the trained model
dump(rf, 'random_forest_model.joblib')
print("Model saved as 'random_forest_model.joblib'")

# Step 6: Load the trained model
loaded_rf = load('random_forest_model.joblib')
print("Model loaded successfully!")

# Step 7: Evaluate the loaded model
y_pred = loaded_rf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d", xticklabels=wine.target_names, yticklabels=wine.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

import pandas as pd
# Load the dataset

file_id = "1Uxk0sDwNBGxQLEijyffwPtTpKoFawsxU"
file_url = f"https://drive.google.com/uc?id={file_id}"

df = pd.read_csv(file_url)
print(len(df))